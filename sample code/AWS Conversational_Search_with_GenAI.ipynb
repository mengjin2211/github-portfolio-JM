{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d41ec6",
   "metadata": {},
   "source": [
    "# Conversational Search with GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03874004-bd68-4fac-a05a-3834986a8e89",
   "metadata": {},
   "source": [
    "## Step 1: Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34490d00-970a-49ac-97f5-1999ff4ca03f",
   "metadata": {},
   "source": [
    "Install required library such as [OpenSearch client library](https://opensearch.org/docs/1.0/clients/python/), [LangChain](https://python.langchain.com/docs/get_started), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06184986",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade sagemaker==2.186.0 \n",
    "%pip install opensearch-py==2.3.1\n",
    "%pip install wikipedia unstructured transformers==4.33.2\n",
    "%pip install langchain==0.0.308 #0.0.293\n",
    "%pip install boto3==1.28.59\n",
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e422af0-3961-4e99-9f90-f46d3cf78f0b",
   "metadata": {},
   "source": [
    "Initialize SageMaker, and Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d0256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "aws_account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad970f25-32cf-4bd8-bfa4-f13e361a5b20",
   "metadata": {},
   "source": [
    "Get Cloud Formation stack output variables\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635d4ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "region = aws_region\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "kms = boto3.client('secretsmanager')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"semantic-search\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "aos_credentials = json.loads(kms.get_secret_value(SecretId=outputs['OpenSearchSecret'])['SecretString'])\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926ca92-3373-47a7-973e-3136ebfa2370",
   "metadata": {},
   "source": [
    "## Step 2: Verify deployed endpoint for embedding and content generation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f49640d",
   "metadata": {},
   "source": [
    "### Get endpoint for embedding\n",
    "\n",
    "---\n",
    "This is SageMaker Endpoint with GPT-J 6B parameters model to convert text into vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5962e3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_endpoint_name=outputs['EmbeddingEndpointName']\n",
    "print(embedding_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5980c841-860a-46af-a3e2-2bf612c0134d",
   "metadata": {},
   "source": [
    "Endpoint should be in `InService` status to be able to serve requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef415501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\", aws_region)\n",
    "\n",
    "describe_embedding_endpoint_response = sm_client.describe_endpoint(EndpointName=embedding_endpoint_name)\n",
    "\n",
    "while describe_embedding_endpoint_response[\"EndpointStatus\"] == 'Creating':\n",
    "    time.sleep(15)\n",
    "    print('.', end='')\n",
    "    describe_embedding_endpoint_response = sm_client.describe_endpoint(EndpointName=embedding_endpoint_name)\n",
    "print(describe_embedding_endpoint_response[\"EndpointStatus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b48027",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get endpoint for content generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecca035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm_endpoint_name=outputs['LLMEndpointName']\n",
    "print(llm_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2214c-3487-45a8-babf-d4bf3b3ff021",
   "metadata": {},
   "source": [
    "Verify embedding endpoint is ready (It should show the status as `InService`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d029d7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\", aws_region)\n",
    "\n",
    "describe_llm_endpoint_response = sm_client.describe_endpoint(EndpointName=llm_endpoint_name)\n",
    "\n",
    "while describe_llm_endpoint_response[\"EndpointStatus\"] == 'Creating':\n",
    "    time.sleep(15)\n",
    "    print('.', end='')\n",
    "    describe_llm_endpoint_response = sm_client.describe_endpoint(EndpointName=llm_endpoint_name)\n",
    "print(describe_embedding_endpoint_response[\"EndpointStatus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b747507b",
   "metadata": {},
   "source": [
    "### Test embedding endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96eb56-b15e-4c46-9412-5d59ad09ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "\n",
    "\n",
    "class TestContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        if len(embeddings) == 1:\n",
    "            return [embeddings[0]]\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "test_content_handler = TestContentHandler()\n",
    "\n",
    "test_embeddings = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=embedding_endpoint_name,\n",
    "    region_name=aws_region,\n",
    "    content_handler=test_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb84f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# return first five elements of the embdedding\n",
    "print(test_embeddings.embed_documents([\"Hello World\"])[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa760ac7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test LLM endpoint\n",
    "Alternatively we can also use AWS Boto3 library to query SageMaker endpoint as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86de9ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name, content_type=\"application/json\"):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "#method used to parse the inference model's response. we pass it as part of the model's config\n",
    "def parse_response_model(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return [gen[\"generated_text\"] for gen in model_predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0c128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = \"Hello OpenSearch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940794c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": question,\n",
    "    \"parameters\":{\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 100,\n",
    "        \"top_p\": 0.5,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": True,\n",
    "        \"temperature\": 0.9\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de27ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_response = query_endpoint_with_json_payload(\n",
    "    json.dumps(payload).encode(\"utf-8\"), endpoint_name=llm_endpoint_name\n",
    ")\n",
    "\n",
    "generated_texts = parse_response_model(query_response)\n",
    "\n",
    "print(f\"The generated output is: {generated_texts[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c4d59-f74f-4636-9e81-754a49bb2fef",
   "metadata": {},
   "source": [
    "## Step 3: Test LLM without context information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f1457",
   "metadata": {},
   "source": [
    "### 3.1 Test SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c089f940",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from typing import Dict\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import DynamoDBChatMessageHistory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    \n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        #print(\"Prompt Input:\\n\" + input_str)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        #print(\"LLM generated text:\\n\" + response_json[0][\"generated_text\"])\n",
    "        return response_json[0][\"generated_text\"]\n",
    "    \n",
    "\n",
    "content_handler = ContentHandler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4247d17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 100,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.9\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb236e6-e242-4d1a-bce3-e2f00409035a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_hallucination=SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=params,\n",
    "        content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to determine shard and data node counts for OpenSearch?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa531974-2eae-4d68-92c0-0a474a80a21d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Question is:\" + question)\n",
    "generated_result = llm_hallucination(question)\n",
    "print(generated_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16dcaa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Question is:\" + question)\n",
    "generated_result = llm_hallucination(question)\n",
    "print(generated_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31632621",
   "metadata": {},
   "source": [
    "### 3.2 Test Bedrock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_bedrock_available=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "bedrock_region=\"us-west-2\"\n",
    "\n",
    "#boto3_bedrock = boto3.client(service_name=\"bedrock-runtime\", endpoint_url=f\"https://bedrock-runtime.{bedrock_region}.amazonaws.com\")\n",
    "boto3_bedrock = boto3.client(service_name=\"bedrock-runtime\", config=Config(region_name=bedrock_region))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffeedad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "bedrock_llm_hallucination = Bedrock(model_id=\"anthropic.claude-instant-v1\", client=boto3_bedrock)\n",
    "bedrock_llm_hallucination.model_kwargs = {\"max_tokens_to_sample\":1204,\"temperature\":0.9,\"top_k\":250,\"top_p\":1,\"stop_sequences\":[\"\\\\n\\\\nHuman:\"]}\n",
    "\n",
    "if is_bedrock_available:\n",
    "    bedrock_result = bedrock_llm_hallucination(question)\n",
    "    print(bedrock_result)\n",
    "else:\n",
    "    print(\"Bedrock is unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a9c1e",
   "metadata": {},
   "source": [
    "## Step 4: Test LLM with context information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d02bf",
   "metadata": {},
   "source": [
    "### 4.1 Test SageMaker with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ec5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_context_params = {\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 100,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.01\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_context=SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=with_context_params,\n",
    "        content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_and_question=\"\"\"\n",
    "Answer the question with with the content in the Context.\n",
    "Context: \n",
    "By default in OpenSearch Service, each index is divided into five primary shards and one replica (total of 10 shards). This behavior differs from open source OpenSearch, which defaults to one primary and one replica shard. Because you can't easily change the number of primary shards for an existing index, you should decide about shard count before indexing your first document.\n",
    "The overall goal of choosing a number of shards is to distribute an index evenly across all data nodes in the cluster. However, these shards shouldn't be too large or too numerous. A general guideline is to try to keep shard size between 10–30 GiB for workloads where search latency is a key performance objective, and 30–50 GiB for write-heavy workloads such as log analytics.\n",
    "Large shards can make it difficult for OpenSearch to recover from failure, but because each shard uses some amount of CPU and memory, having too many small shards can cause performance issues and out of memory errors. In other words, shards should be small enough that the underlying OpenSearch Service instance can handle them, but not so small that they place needless strain on the hardware.\n",
    "For example, suppose you have 66 GiB of data. You don't expect that number to increase over time, and you want to keep your shards around 30 GiB each. Your number of shards therefore should be approximately 66 * 1.1 / 30 = 3. \n",
    "\n",
    "Question:How to determine shard and data node counts for OpenSearch?\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "generated_result = llm_with_context(context_and_question)\n",
    "print(generated_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e258274",
   "metadata": {},
   "source": [
    "## Step 5: Select SageMaker or Bedrock used for embedding and content generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "\n",
    "llm_selection = [\n",
    "    \"SageMaker\",\n",
    "    \"Bedrock\",\n",
    "]\n",
    "\n",
    "llm_dropdown = Dropdown(\n",
    "    options=llm_selection,\n",
    "    value=\"SageMaker\",\n",
    "    description=\"Select a LLM\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(llm_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276082d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_category = llm_dropdown.value\n",
    "\n",
    "if not is_bedrock_available:\n",
    "    llm_category = \"SageMaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cacf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You selected {0} as LLM\".format(llm_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacdeaa9",
   "metadata": {},
   "source": [
    "## Step 6: Load documents with LangChain document loader and store vector into OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0369d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
    "\n",
    "urls = [\"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/bp.html\",\n",
    "        \"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/sizing-domains.html\", \n",
    "        \"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/petabyte-scale.html\",\n",
    "        \"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/managedomains-dedicatedmasternodes.html\",\n",
    "        \"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/cloudwatch-alarms.html\"]\n",
    "url_loader = UnstructuredURLLoader(urls=urls)\n",
    "#url_loader = SeleniumURLLoader(urls=urls)\n",
    "url_texts = url_loader.load_and_split(text_splitter=text_splitter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e53165-93fd-4434-a653-40ba1e98c67d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_splits = url_texts\n",
    "print(all_splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede13e1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "\n",
    "auth = (aos_credentials['username'], aos_credentials['password'])\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a03874-5a51-456e-b36c-e31771d73ad0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LangChain embedding endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059dd4b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Callable\n",
    "import json\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.schema import Document\n",
    "\n",
    "class BulkSagemakerEndpointEmbeddings(SagemakerEndpointEmbeddings):\n",
    "        def embed_documents(\n",
    "            self, texts: List[str], chunk_size: int = 5\n",
    "        ) -> List[List[float]]:\n",
    "            results = []\n",
    "            _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "\n",
    "            for i in range(0, len(texts), _chunk_size):\n",
    "                response = self._embedding_func(texts[i:i + _chunk_size])\n",
    "                results.extend(response)\n",
    "            return results\n",
    "        \n",
    "class EmbeddingContentHandler(EmbeddingsContentHandler):\n",
    "        content_type = \"application/json\"\n",
    "        accepts = \"application/json\"\n",
    "\n",
    "        def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "\n",
    "            input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "            return input_str.encode('utf-8') \n",
    "\n",
    "        def transform_output(self, output: bytes) -> str:\n",
    "\n",
    "            response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "            embeddings = response_json[\"embedding\"]\n",
    "            if len(embeddings) == 1:\n",
    "                return [embeddings[0]]\n",
    "            return embeddings\n",
    "\n",
    "print(embedding_endpoint_name)\n",
    "sagemaker_embeddings = BulkSagemakerEndpointEmbeddings( \n",
    "            endpoint_name=embedding_endpoint_name,\n",
    "            region_name=aws_region, \n",
    "            content_handler=EmbeddingContentHandler())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56027a",
   "metadata": {},
   "source": [
    "### Bedrock embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b84fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id='amazon.titan-embed-text-v1',client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57297dfc-3816-4039-8a61-35e0ff1965cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### OpenSearch vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49401f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "match llm_category:\n",
    "    case \"SageMaker\":\n",
    "        embeddings = sagemaker_embeddings\n",
    "    case \"Bedrock\":\n",
    "        embeddings = bedrock_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe272ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c04e0",
   "metadata": {},
   "source": [
    "Initialize OpenSearch index name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index_name = 'opensearch_kb_vector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a499a93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#aos_client.indices.delete(index=embedding_index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d425b-0aa0-4e40-9be8-db082b3c5e0c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "os_domain_ep = 'https://'+aos_host\n",
    "\n",
    "if len(all_splits) > 500:\n",
    "    for i in range(0, len(all_splits), 500):\n",
    "        start = i\n",
    "        end = i+500\n",
    "        if end > len(all_splits):\n",
    "            end = len(all_splits)-1\n",
    "        docs = all_splits[start:end]\n",
    "        OpenSearchVectorSearch.from_documents(\n",
    "            index_name = embedding_index_name,\n",
    "            documents=docs,\n",
    "            embedding=embeddings,\n",
    "            opensearch_url=os_domain_ep,\n",
    "            http_auth=auth\n",
    "        )\n",
    "        print(f\"ingest documents from {start} to {end}\", start, end)\n",
    "else:\n",
    "    OpenSearchVectorSearch.from_documents(\n",
    "            index_name = embedding_index_name,\n",
    "            documents=all_splits,\n",
    "            embedding=embeddings,\n",
    "            opensearch_url=os_domain_ep,\n",
    "            http_auth=auth\n",
    "        )\n",
    "    print(f\" completed documents ingestion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a0d643",
   "metadata": {},
   "source": [
    "The the OpenSearch index detailed information. Pay attention to `dimension` field value when you choose different LLM. For SageMaker, we use GPT-J enmbedding model whose dimension is 4096. For Bedrock, we use Titan Embedding modele whose dimension is 1536."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369f552-2b2c-49fe-955b-fd814183b720",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=embedding_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee94d29f",
   "metadata": {},
   "source": [
    "Initialize OpenSearch index name whose settings are customized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "customized_embedding_index_name = 'customized_opensearch_kb_vector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c29f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aos_client.indices.delete(index=customized_embedding_index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610f4d3-1aa7-4a03-a668-90ebd4874d9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "OpenSearchVectorSearch.from_documents(\n",
    "            index_name = customized_embedding_index_name,\n",
    "            documents=all_splits,\n",
    "            embedding=embeddings,\n",
    "            opensearch_url=os_domain_ep,\n",
    "            http_auth=auth,\n",
    "            engine=\"faiss\",\n",
    "            space_type=\"innerproduct\",\n",
    "            ef_construction=256,\n",
    "            m=48,\n",
    "        )\n",
    "print(f\"ingest documents into customized knn index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673dbddf",
   "metadata": {},
   "source": [
    "Get OpenSearch index detailed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa8d63-6a13-42dd-9123-8d6ef19218a9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=customized_embedding_index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110d127-b60b-45e1-a178-37fd175537ac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimiliarOpenSearchVectorSearch(OpenSearchVectorSearch):\n",
    "    \n",
    "    def relevance_score(self, distance: float) -> float:\n",
    "        return distance\n",
    "    \n",
    "    def _select_relevance_score_fn(self) -> Callable[[float], float]:\n",
    "        return self.relevance_score\n",
    "    \n",
    "\n",
    "open_search_vector_store = SimiliarOpenSearchVectorSearch(\n",
    "                                    index_name=embedding_index_name,\n",
    "                                    embedding_function=embeddings,\n",
    "                                    opensearch_url=os_domain_ep,\n",
    "                                    http_auth=auth\n",
    "                                    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b644b724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_ = open_search_vector_store.similarity_search_with_score(question, k=5)\n",
    "\n",
    "print(\"found document number:\" + str(len(docs_)))\n",
    "\n",
    "print(\"opensearch results:\\n\")\n",
    "for doc in docs_:\n",
    "    print(doc)\n",
    "    print(\"\\n-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0e71e-bf17-4f6e-aaff-be56a874c212",
   "metadata": {},
   "source": [
    "## Step 7: Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55211dcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sagemaker_retriever = open_search_vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        'k': 5,\n",
    "        'score_threshold': 0.62\n",
    "    }\n",
    ")\n",
    "\n",
    "bedrock_retriever = open_search_vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        'k': 5,\n",
    "        'score_threshold': 0.005\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b96738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "match llm_category:\n",
    "    case \"SageMaker\":\n",
    "        retriever = sagemaker_retriever\n",
    "    case \"Bedrock\":\n",
    "        retriever = bedrock_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29542ec7-0296-4f3e-b89d-fe9064665960",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_params = {\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 200,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0001\n",
    "        }\n",
    "\n",
    "sagemaker_llm=SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=params,\n",
    "        content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf77ab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_params = {\n",
    "    \"max_tokens_to_sample\":2048,\n",
    "    \"temperature\":0.0001,\n",
    "    \"top_k\":250,\n",
    "    \"top_p\":1,\n",
    "    \"stop_sequences\":[\"\\\\n\\\\nHuman:\"]\n",
    "}\n",
    "\n",
    "bedrock_titan_llm = Bedrock(model_id=\"anthropic.claude-instant-v1\", client=boto3_bedrock)\n",
    "bedrock_titan_llm.model_kwargs = bedrock_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7422be1",
   "metadata": {},
   "source": [
    "### Select content generation LLM, SageMaker or Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22732f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "match llm_category:\n",
    "    case \"SageMaker\":\n",
    "        llm = sagemaker_llm\n",
    "    case \"Bedrock\":\n",
    "        llm = bedrock_titan_llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92a6d1",
   "metadata": {},
   "source": [
    "Show the the LLM for content generation to be used, SageMaker or Bedrock selected above\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3bba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb9bf32",
   "metadata": {},
   "source": [
    "Define `RetrievalQA` Chain with SageMaker or Bedrock LLM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a1e14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\" #stuff, refine, map_reduce, and map_rerank\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869cfe8-963f-4053-aa00-22f7ff76a132",
   "metadata": {},
   "source": [
    "Use RAG to generate answer to the same question before. Compare the content generated with RAG and LLM without context.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feaca4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Question is:\" + question)\n",
    "\n",
    "#langchain.debug=False\n",
    "result = qa({\"query\": question})\n",
    "\n",
    "print(\"result:\" + result[\"result\"])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774aac71-7220-4205-bf2d-1ed253dd5b1e",
   "metadata": {},
   "source": [
    "### Use customized prompt for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0959e688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "template2 = \"\"\"Answer the question as truthfully as possible by using the provided informaiton in >>CONTEXT<<. If the answer is not contained within the >>CONTEXT<<, respond with \"I can't answer that\".\n",
    "\n",
    ">>CONTEXT<<:\n",
    "{context}\n",
    "\n",
    ">>QUESTION<<:\n",
    "{question}\n",
    "\n",
    ">>Answer<<:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_template2 = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=template2\n",
    ")\n",
    "\n",
    "qa_with_prompt = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template2}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8517d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "#langchain.debug = True\n",
    "langchain.debug = False\n",
    "\n",
    "print(\"Question is:\" + question)\n",
    "result = qa_with_prompt({\"query\": question})\n",
    "\n",
    "print(\"\\n### Generated result:\" + result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5710a4f-5652-4a2d-a126-0ac8e10aceef",
   "metadata": {},
   "source": [
    "### Return source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba727f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qa_with_source = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131847b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Question is:\" + question)\n",
    "result = qa_with_source({\"query\": question})\n",
    "\n",
    "print(\"result:\" + result[\"result\"])\n",
    "print(\"\\n\\n===========================\")\n",
    "print(\"\\nsource documents:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(doc)\n",
    "    print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c88750-e300-45f9-988f-3e238e72f84f",
   "metadata": {},
   "source": [
    "## Step 8: Conversational search by memorizing the history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78531186",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamo = boto3.client('dynamodb')\n",
    "\n",
    "history_table_name = 'conversation-history-memory'\n",
    "\n",
    "try:\n",
    "    response = dynamo.describe_table(TableName=history_table_name)\n",
    "    print(\"The table \"+history_table_name+\" exists\")\n",
    "except dynamo.exceptions.ResourceNotFoundException:\n",
    "    print(\"The table \"+history_table_name+\" does not exist\")\n",
    "    \n",
    "    dynamo.create_table(\n",
    "    TableName=history_table_name,\n",
    "    AttributeDefinitions=[\n",
    "        {\n",
    "            'AttributeName': 'SessionId',\n",
    "            'AttributeType': 'S',\n",
    "        }\n",
    "    ],\n",
    "    KeySchema=[\n",
    "        {\n",
    "            'AttributeName': 'SessionId',\n",
    "            'KeyType': 'HASH',\n",
    "        }\n",
    "    ],\n",
    "    ProvisionedThroughput={\n",
    "        'ReadCapacityUnits': 5,\n",
    "        'WriteCapacityUnits': 5,\n",
    "    }\n",
    "    )\n",
    "\n",
    "    response = dynamo.describe_table(TableName=history_table_name) \n",
    "    \n",
    "    while response[\"Table\"][\"TableStatus\"] == 'CREATING':\n",
    "        time.sleep(1)\n",
    "        print('.', end='')\n",
    "        response = dynamo.describe_table(TableName=history_table_name) \n",
    "\n",
    "    print(\"\\ndynamo DB Table, '\"+response['Table']['TableName']+\"' is created\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea68c2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddb_table_name = \"conversation-history-memory\"\n",
    "session_id = str(uuid4())\n",
    "chat_memory = DynamoDBChatMessageHistory(\n",
    "        table_name=ddb_table_name,\n",
    "        session_id=session_id\n",
    "    )\n",
    "\n",
    "messages = chat_memory.messages\n",
    "\n",
    "# Maintains immutable sessions\n",
    "# If previous session was present, create\n",
    "# a new session and copy messages, and \n",
    "# generate a new session_id \n",
    "if messages:\n",
    "    session_id = str(uuid4())\n",
    "    chat_memory = DynamoDBChatMessageHistory(\n",
    "        table_name=ddb_table_name,\n",
    "        session_id=session_id\n",
    "    )\n",
    "    # This is a workaround at the moment. Ideally, this should\n",
    "    # be added to the DynamoDBChatMessageHistory class\n",
    "    try:\n",
    "        messages = messages_to_dict(messages)\n",
    "        chat_memory.table.put_item(\n",
    "            Item={\"SessionId\": session_id, \"History\": messages}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "memory = ConversationBufferMemory(chat_memory=chat_memory, memory_key=\"chat_history\", return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb0a3e-40a1-4927-8833-5f185410eef8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "params = {\n",
    "        \"max_length\": 2048,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 200,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0001\n",
    "        }\n",
    "\n",
    "sagemaker_llm=SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=params,\n",
    "        content_handler=content_handler,\n",
    ")\n",
    "\n",
    "condense_template = \"\"\"system: generate one standalone question.\n",
    "Given the following conversation between <chat-history> and </chat-history> \n",
    "and follow up question between <follow-up-question> and </follow-up-question>, \n",
    "rephrase the follow up question to be a standalone question in its original language. \n",
    "The standalone question will only contains one sentence and it must end with '?'\n",
    "\n",
    "<chat-history>\n",
    "{chat_history}\n",
    "</chat-history>\n",
    "\n",
    "<follow-up-question>\n",
    "{question}\n",
    "</follow-up-question>\n",
    "\n",
    "standalone question:\n",
    "\"\"\"\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(condense_template)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "match llm_category:\n",
    "    case \"SageMaker\":\n",
    "        llm = sagemaker_llm\n",
    "    case \"Bedrock\":\n",
    "        llm = bedrock_titan_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bc823",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_memory = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_template2},\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc1671-79f2-4056-8248-6eff1e3c16fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = qa_with_memory(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02d8f4-4a23-412f-a76d-41e75037ad9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(\"result:\" + str(result))\n",
    "print(\"\\nAnswer:\\n\" + str(result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5cdd8c-a04d-41fe-99e6-33b531015260",
   "metadata": {},
   "source": [
    "### Second question\n",
    "Try to ask one more question, `ConversationalRetrievalChain` will use the first question, first question's answer and second question as prompt to LLM to generate new question. The prompt to LLM is like following:\n",
    "\n",
    "```python\n",
    "\n",
    "_template = \"\"\"system: generate one standalone question.\n",
    "Given the following conversation between <chat-history> and </chat-history> \n",
    "and follow up question between <follow-up-question> and </follow-up-question>, \n",
    "rephrase the follow up question to be a standalone question in its original language. \n",
    "The standalone question will only contains one sentence and it must end with '?'\n",
    "\n",
    "<chat-history>\n",
    "{chat_history}\n",
    "</chat-history>\n",
    "\n",
    "<follow-up-question>\n",
    "{question}\n",
    "</follow-up-question>\n",
    "\n",
    "standalone question:\n",
    "```\n",
    "\n",
    "After get the new question from LLM, it will search relevant document from OpenSearch vector store and get relevant documents, then combine the new question and relevant documents as prompt to go through RAG process. The prompt to LLM is like following:\n",
    "\n",
    "```python\n",
    "prompt_template = \"\"\"Answer the question as truthfully as possible by using the provided informaiton in >>CONTEXT<<. If the answer is not contained within the >>CONTEXT<<, respond with \"I can't answer that\".\n",
    "\n",
    ">>CONTEXT<<:\n",
    "{context}\n",
    "\n",
    ">>QUESTION<<:\n",
    "{question}\n",
    "\n",
    ">>Answer<<:\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "In summary, `ConversationalRetrievalChain` will call LLM twice:\n",
    "1. Use history question, history answer and latest question as prompt to generate new question\n",
    "2. Use new question generated in the first step, query relevant documents. Combine relevant documents and new question as prompt to LLM to generate answer.\n",
    "\n",
    "You can also see the verbose message like following:\n",
    "\n",
    "---\n",
    "\n",
    "### First call to LLM:\n",
    "\n",
    "![generate new question](../image/module8/conversation-new-question.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Second call to LLM:\n",
    "\n",
    "![generate final answer](../image/module8/conversation-final-answer.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e63fc-6c54-479b-b7c5-8310a16896a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_following_question = 'if my data growth is very fast'\n",
    "second_result = qa_with_memory(second_following_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a8099a-a3be-4996-9852-97a6282d85e4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"second answer:\" + str(second_result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a07d06d-eabf-425a-a9ed-808abf76a271",
   "metadata": {},
   "source": [
    "### Return source document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3cc063-ede9-4bed-b780-912a4cbc583c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can also include source documents so that we can know where the content information are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3d864-222a-41f7-a88f-d00754cacda1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "session_id = str(uuid4())\n",
    "chat_memory = DynamoDBChatMessageHistory(\n",
    "        table_name=ddb_table_name,\n",
    "        session_id=session_id\n",
    "    )\n",
    "\n",
    "memory_for_source = ConversationBufferMemory(chat_memory=chat_memory,memory_key=\"chat_history\")\n",
    "\n",
    "qa_with_memory_and_source = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    retriever=retriever,\n",
    "    condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_template2},\n",
    "    return_source_documents=True, \n",
    "    verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb89fbf6-b500-46ef-af80-006ab3c610e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "result = qa_with_memory_and_source({\"question\": question,\"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da474e5-a8d9-4739-a135-5a686146dc7e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nAnswer:\\n\" + str(result[\"answer\"]))\n",
    "print(\"\\nSource Documents:\\n\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(str(doc))\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef6aa7c-6e9f-41fa-9a7f-fc29b288a1ca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = [(question, result[\"answer\"])]\n",
    "second_query = second_following_question\n",
    "second_result = qa_with_memory_and_source({\"question\": second_query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8965962-cd1d-481b-bed2-e3f475252814",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nAnswer:\\n\" + str(second_result[\"answer\"]))\n",
    "print(\"\\nSource Documents:\\n\")\n",
    "for doc in second_result[\"source_documents\"]:\n",
    "    print(str(doc))\n",
    "    print(\"--------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jin",
   "language": "python",
   "name": "jin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
